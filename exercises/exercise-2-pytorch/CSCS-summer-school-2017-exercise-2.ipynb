{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Deep learning\n",
    "\n",
    "CSCS-ICS-DADSi Summer School: Accelerating Data Science with HPC\n",
    "September 4 - 6, 2017\n",
    "Swiss National Supercomputing Centre\n",
    "\n",
    "Mainly inspired from:\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the PyTorch deep learning framework (http://pytorch.org/) in this exercise.\n",
    "\n",
    "PyTorch is a Python-based reincarnation of the lua-based Torch framework (http://torch.ch/), with features such as dynamic graphs and general-purpose automatic differentiation.\n",
    "\n",
    "Please visit the PyTorch website for more information and tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors, or multi-dimensional arrays, are the basic unit in PyTorch. Let's create a 5x3 matrix as a 2-dimensional Tensor. This will default to a `FloatTensor`, a tensor of single-precision (32-bits) floating point numbers. The tensor will be initialized with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-22 *\n",
      "  3.5701  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0018  0.0000\n",
      "  0.0018  0.0000  0.0018\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "It is common to have higher-dimensional tensors, to hold all aspects of your data together, such as a 3x200x200 tensor holding the R,G,B channels of an image of 200x200 pixels, or a 100x3x200x200 tensor to hold a batch of 100 such images.\n",
    "\n",
    "We use slicing operations to view specific subtensors of such a tensor. Note that PyTorch uses zero-based indexing and row-major memory ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 200, 200])\n",
      "torch.Size([200, 200])\n"
     ]
    }
   ],
   "source": [
    "# 100 images of 3 channels of size 200x200\n",
    "x = Tensor(100,3,200,200)\n",
    "print(x.size())\n",
    "\n",
    "# Second channel of the first image\n",
    "y = x[0,1]\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy conversion\n",
    "\n",
    "You might have noticed that Tensors are very similar to Numpy's ndarrays. In fact you can easily convert between ndarrays and Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 200, 200)\n",
      "torch.Size([100, 3, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "x_np = x.numpy()\n",
    "print(x_np.shape)\n",
    "\n",
    "x = torch.from_numpy(x_np)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on GPU usage\n",
    "\n",
    "PyTorch is frequently used with a CUDA backend on a GPU. One would convert between tensors and CUDA tensors by simply calling\n",
    "\n",
    "`x.cuda()`\n",
    "\n",
    "`x.cpu()`\n",
    "\n",
    "which transfer data to and from a GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implements reverse-mode automatic differentiation (AD). For using AD features, we need the `Variable` class from  `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For automatically evaluating derivatives with AD, we need to wrap tensors within `Variable` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3.5700e-22\n",
      " 4.5843e-41\n",
      " 1.0178e-36\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(3) # A vector of length three\n",
    "x = Variable(x) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `Variable`s, obtaining a gradient of a scalar-valued function is straightforward. Let's use the regular Python language features to define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = torch.log(x[0]) + torch.sin(x[1])\n",
    "    return x[2] * torch.exp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the gradient of `f(x)` at `x = [2,3,4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 9.2125\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.6063\n",
      "-9.1203\n",
      " 2.3031\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(Tensor([2,3,4]), requires_grad=True)\n",
    "y = f(x)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get the function evaluated `f(2,3,4)` as `9.2125` and the gradient `f'(2,3,4)` as `[4.6063, -9.1203, 2.3031]`.\n",
    "\n",
    "Also note the use of `requires_grad = True` for tagging the `Variable` `x` for being differentiated with respect to. This is for efficiency reasons, allowing PyTorch to ignore the bookkeeping for `Variable`s whose derivatives we don't need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two parts, namely defining and using `Tensor`s and `Variable`s and being able to obtain derivatives, give us everything needed for constructing and training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important point to note is that **neural networks are just functions, and there is nothing special about them.**\n",
    "\n",
    "A neural network is a series of linear algebra operations interleaved with non-linear transformations. Let's create a single feed-forward layer with two neurons connected to four inputs (note that this omits the bias term for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.0000\n",
      "-0.9923\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_layer(weights, inputs):\n",
    "    outputs = torch.mm(weights , inputs)\n",
    "    outputs = torch.tanh(outputs)\n",
    "    return outputs\n",
    "\n",
    "W = Variable(torch.randn(2,4), requires_grad=True) # A 2x4 weight matrix\n",
    "x = Variable(Tensor([[1],[2],[3],[4]])) # A column vector of length four\n",
    "\n",
    "y = run_layer(W, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Training this neural network layer would simply mean taking the derivative of a loss function at its output with respect to its trainable weights (the matrix `W` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3.5573\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.2821 -0.5643 -0.8464 -1.1285\n",
      "-0.0001 -0.0003 -0.0004 -0.0005\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def loss(outputs, targets):\n",
    "    return torch.norm(outputs - targets)\n",
    "\n",
    "W = Variable(torch.randn(2,4), requires_grad=True) # A 2x4 weight matrix\n",
    "x = Variable(Tensor([[1],[2],[3],[4]])) # A column vector of length four\n",
    "\n",
    "loss = loss(run_layer(W, x), Variable(Tensor([1,1,1])))\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "W_grad = W.grad\n",
    "print(W_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "We would then iteratively update the weights in a gradient-based update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.5265  0.0008 -0.7279  0.8278\n",
      " 0.0988 -0.2791 -0.9714 -0.5097\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "W = W - learning_rate * W_grad\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7311\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(Tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the gradient of a neural network with respect to its trainable parameters (weights), we can optimize it with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, unless you are implementing new neural network architectures, you would use the existing neural network building blocks provided by the `torch.nn` module, which internally work similar to the basic example we covered above.\n",
    "\n",
    "We will now define a convolutional neural network for image recognition on the MNIST dataset and train and test it, using the regular PyTorch workflow. The MNIST dataset is a collection of 60,000 images of handwritten digits labeled as belonging to one of the ten digit categories.\n",
    "\n",
    "First we load the MNIST dataset. We also apply preprocessing for normalizing data.\n",
    "\n",
    "We divide the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will help us visualize data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy().astype(float)\n",
    "    plt.imshow(npimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a few images and corresponding ground truth labels from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAYL329OsCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaab37e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 12\n",
    "imshow(train_loader.dataset.train_data[index])\n",
    "print('Ground truth label: ', train_loader.dataset.train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth label:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaab090b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 150\n",
    "imshow(train_loader.dataset.train_data[index])\n",
    "print('Ground truth label: ', train_loader.dataset.train_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our neural network composed of two convolutional layers followed by max-pooling layers, a dropout layer, and two fully-connected (linear) layers.\n",
    "\n",
    "Note that we only need to \n",
    "- create the layer modules (in the `__init__` constructor) and \n",
    "- define the function for the forward run of the network (in the `forward` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # The definition of neural network layers\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x): # The function for forward evaluation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the stochastic gradient descent (SGD) optimization routine provided by the `torch.optim` module. We use a learning rate of `0.0001` and a momentum of `0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the neural network for four epochs. \n",
    "\n",
    "As training progresses, you will see that the classification accuracy in the test set will keep rising. At the end of four epochs, we will have an accuracy of around 85% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304117\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.337224\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.316170\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.293122\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.315565\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.285456\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.291956\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.239765\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.266539\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.248861\n",
      "\n",
      "Test set: Average loss: 2.2314, Accuracy: 4130/10000 (41%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.253264\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.227257\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.259247\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.231233\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.170665\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.232805\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.160343\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.157897\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.168804\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.082378\n",
      "\n",
      "Test set: Average loss: 2.0197, Accuracy: 5947/10000 (59%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.042473\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.104316\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.099521\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.946665\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.946715\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.826851\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.874389\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.832521\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.914814\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.796982\n",
      "\n",
      "Test set: Average loss: 1.4319, Accuracy: 7157/10000 (72%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.576520\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.522017\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.419768\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.491702\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.582483\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.243263\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.410439\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.445273\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.299558\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.317284\n",
      "\n",
      "Test set: Average loss: 0.8338, Accuracy: 8115/10000 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9510  0.0001  0.0165  0.0176  0.0002  0.0023  0.0080  0.0030  0.0008  0.0004\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiL8QFCEHwiK8ANBEX4gqP8HDRWPK3cmw2kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaab06a630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "index = 3\n",
    "a = list(itertools.islice(test_loader, index))\n",
    "test_image = a[0][0][0]\n",
    "test_label = a[0][1][0]\n",
    "\n",
    "test_output = model(Variable(test_image.unsqueeze(0)))\n",
    "values, indices = torch.max(test_output, 0)\n",
    "imshow(test_image[0])\n",
    "print(F.softmax(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0072  0.0146  0.0207  0.0102  0.0111  0.0130  0.0035  0.8462  0.0238  0.0498\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADRZJREFUeJzt3W2MXOV5xvHr8rLYwZBgY9hYxorz4lBR1Jp0a9rGTYlo\nIoNQTT5A40jIkSycD9AGKY2KaKPyoVVRaUJpUlFtihPTUpJKCcWRUBpiIVlRkGGhBoPdxECNsGu8\nJBhhSjFr++6HPUYL7DyzzJyZM8v9/0mrnTn3ebk98rVn5jwz8zgiBCCfeU03AKAZhB9IivADSRF+\nICnCDyRF+IGkCD+QFOEHkiL8QFKn9PNgp3p+LNDCfh4SSOU1/a9ej6Oezbpdhd/2Wkm3SRqS9E8R\ncXNp/QVaqIt8STeHBFCwI7bNet2On/bbHpL0D5IulXS+pPW2z+90fwD6q5vX/KslPRURz0TE65K+\nI2ldPW0B6LVuwr9M0nPT7u+vlr2J7U22x22PT+poF4cDUKeeX+2PiLGIGI2I0WHN7/XhAMxSN+E/\nIGn5tPvnVssAzAHdhP9hSSttf9D2qZI+K2lrPW0B6LWOh/oi4pjt6yT9h6aG+jZHxJO1dQagp7oa\n54+I+yTdV1MvAPqIt/cCSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQI\nP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGk\nCD+QVFez9NreJ+mIpOOSjkXEaB1NAei9rsJf+WRE/KKG/QDoI572A0l1G/6Q9CPbj9jeVEdDAPqj\n26f9ayLigO1zJN1v+78iYvv0Fao/CpskaYFO6/JwAOrS1Zk/Ig5Uvyck3SNp9QzrjEXEaESMDmt+\nN4cDUKOOw297oe0zTt6W9GlJT9TVGIDe6uZp/4ike2yf3M+/RsQPa+kKQM91HP6IeEbSr9fYC4A+\nYqgPSIrwA0kRfiApwg8kRfiBpAg/kFQdn+pDw+YtWNC6+NEVXe372KL3FOsbx/69WL/y9F+2rE0c\nf7W47dUb/rhYH3rg0WIdZZz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApxvkHwKE/+p1i/ax1+4v1\npae93LL2rQ/c1VFPJ82Ti/UTijb11pYMld9D8Mvry+8DOOeBYhltcOYHkiL8QFKEH0iK8ANJEX4g\nKcIPJEX4gaQY5x8AF3/+oWL9lvfv6FMng+V3lz1drP+sT328W3HmB5Ii/EBShB9IivADSRF+ICnC\nDyRF+IGk2o7z294s6XJJExFxQbVssaTvSlohaZ+kqyLicO/afHebeO2Mrrb/72Ovtaw9f3xhV/v+\nkz1XFuuv//DsYn2y8E/bee3XO2kJNZnNmf/bkta+ZdkNkrZFxEpJ26r7AOaQtuGPiO2SXnzL4nWS\ntlS3t0i6oua+APRYp6/5RyLiYHX7eUkjNfUDoE+6vuAXESG1/iI325tsj9sen9TRbg8HoCadhv+Q\n7aWSVP2eaLViRIxFxGhEjA5rfoeHA1C3TsO/VdKG6vYGSffW0w6Afmkbftt3S3pQ0nm299veKOlm\nSZ+yvVfS71f3Acwhbcf5I2J9i9IlNfeS1kvXlMfKL/q964r1kZ++1LJ24rE9HfV00iLtbbNGuf4/\nXy7PSYDm8A4/ICnCDyRF+IGkCD+QFOEHkiL8QFJ8dfcAOL7758X62bvL25emwW7aeX9Q/rehOZz5\ngaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApxvnRFZ9S/i+0ZvFTHe/7B7t+rVj/qB7peN/gzA+kRfiB\npAg/kBThB5Ii/EBShB9IivADSTHOj668sPE3i/Vrz/xGy1ppanFJ+pW/e7VYH+TvMZgLOPMDSRF+\nICnCDyRF+IGkCD+QFOEHkiL8QFJtx/ltb5Z0uaSJiLigWnaTpGskvVCtdmNE3NerJtGcUz60oli/\n/Ya/b7OH1ueXS7eXpx7/yGP/2Wbf6MZszvzflrR2huW3RsSq6ofgA3NM2/BHxHZJL/ahFwB91M1r\n/utsP257s+1FtXUEoC86Df/tkj4saZWkg5K+2mpF25tsj9sen9TRDg8HoG4dhT8iDkXE8Yg4Iemb\nklYX1h2LiNGIGB3W/E77BFCzjsJve+m0u5+R9EQ97QDol9kM9d0t6WJJS2zvl/QXki62vUpSSNon\n6Qs97BFAD7QNf0Ssn2HxHT3oBQPo0CeXFusXntr5NeN5BxZ0vC26xzv8gKQIP5AU4QeSIvxAUoQf\nSIrwA0nx1d0ouuKLDxTrwx4q1p+efKVl7SN/VX5vGF/N3Vuc+YGkCD+QFOEHkiL8QFKEH0iK8ANJ\nEX4gKcb5kzu84beL9T9fcnuxPhnHi/XPfeXLLWtnHnmwuC16izM/kBThB5Ii/EBShB9IivADSRF+\nICnCDyTFOH9yL619tVg/HuVP1W/7v9OK9bMePNR638Ut0Wuc+YGkCD+QFOEHkiL8QFKEH0iK8ANJ\nEX4gqbbj/LaXS7pT0oikkDQWEbfZXizpu5JWSNon6aqIONy7VtGJvV+/qFjf84lvFOtPTpZH42/9\n3IY2Dewq19GY2Zz5j0n6UkScL+m3JF1r+3xJN0jaFhErJW2r7gOYI9qGPyIORsSj1e0jkvZIWiZp\nnaQt1WpbJF3RqyYB1O8dvea3vULShZJ2SBqJiINV6XlNvSwAMEfMOvy2T5f0PUnXR8TL02sREZq6\nHjDTdptsj9sen9TRrpoFUJ9Zhd/2sKaCf1dEfL9afMj20qq+VNLETNtGxFhEjEbE6LDm19EzgBq0\nDb9tS7pD0p6I+Nq00lZJJy/1bpB0b/3tAeiV2Xyk9+OSrpa0y/bOatmNkm6W9G+2N0p6VtJVvWkR\n7Qy9970ta1eu2VHcdl6bv/+7j55TPvhDDOXNVW3DHxE/keQW5UvqbQdAv/AOPyApwg8kRfiBpAg/\nkBThB5Ii/EBSfHX3u8DEH/5qy9pfnlP+yG47f/2P64v19+unXe0fzeHMDyRF+IGkCD+QFOEHkiL8\nQFKEH0iK8ANJMc4/Bwyd+b5i/QdfuaVQfU9x2/N+fE2xvvJWxvHfrTjzA0kRfiApwg8kRfiBpAg/\nkBThB5Ii/EBSjPPPBS7/jV4yVB7LLznvlleL9RMd7xmDjjM/kBThB5Ii/EBShB9IivADSRF+ICnC\nDyTVdpzf9nJJd0oakRSSxiLiNts3SbpG0gvVqjdGxH29ahSd+dhDVxfr5z77XJ86waCZzZt8jkn6\nUkQ8avsMSY/Yvr+q3RoRf9u79gD0StvwR8RBSQer20ds75G0rNeNAeitd/Sa3/YKSRdK2lEtus72\n47Y3217UYptNtsdtj0/qaFfNAqjPrMNv+3RJ35N0fUS8LOl2SR+WtEpTzwy+OtN2ETEWEaMRMTqs\n+TW0DKAOswq/7WFNBf+uiPi+JEXEoYg4HhEnJH1T0uretQmgbm3Db9uS7pC0JyK+Nm350mmrfUbS\nE/W3B6BXZnO1/+OSrpa0y/bOatmNktbbXqWp4b99kr7Qkw7Rlff9yxnF+okjR/rUCQbNbK72/0SS\nZygxpg/MYbzDD0iK8ANJEX4gKcIPJEX4gaQIP5AUX909Bxw/fLhYv3zZb7SsLXzjYxjAm3HmB5Ii\n/EBShB9IivADSRF+ICnCDyRF+IGkHBH9O5j9gqRnpy1aIukXfWvgnRnU3ga1L4neOlVnbx+IiLNn\ns2Jfw/+2g9vjETHaWAMFg9rboPYl0VunmuqNp/1AUoQfSKrp8I81fPySQe1tUPuS6K1TjfTW6Gt+\nAM1p+swPoCGNhN/2Wts/s/2U7Rua6KEV2/ts77K90/Z4w71stj1h+4lpyxbbvt/23ur3jNOkNdTb\nTbYPVI/dTtuXNdTbctsP2N5t+0nbX6yWN/rYFfpq5HHr+9N+20OSfi7pU5L2S3pY0vqI2N3XRlqw\nvU/SaEQ0PiZs+xOSXpF0Z0RcUC37G0kvRsTN1R/ORRHxpwPS202SXml65uZqQpml02eWlnSFpM+r\nwceu0NdVauBxa+LMv1rSUxHxTES8Luk7ktY10MfAi4jtkl58y+J1krZUt7do6j9P37XobSBExMGI\neLS6fUTSyZmlG33sCn01oonwL5P03LT7+zVYU36HpB/ZfsT2pqabmcFINW26JD0vaaTJZmbQdubm\nfnrLzNID89h1MuN13bjg93ZrIuJjki6VdG319HYgxdRrtkEarpnVzM39MsPM0m9o8rHrdMbrujUR\n/gOSlk+7f261bCBExIHq94SkezR4sw8fOjlJavV7ouF+3jBIMzfPNLO0BuCxG6QZr5sI/8OSVtr+\noO1TJX1W0tYG+ngb2wurCzGyvVDSpzV4sw9vlbShur1B0r0N9vImgzJzc6uZpdXwYzdwM15HRN9/\nJF2mqSv+T0v6syZ6aNHXhyQ9Vv082XRvku7W1NPASU1dG9ko6SxJ2yTtlfRjSYsHqLd/lrRL0uOa\nCtrShnpbo6mn9I9L2ln9XNb0Y1foq5HHjXf4AUlxwQ9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8\nQFL/D9E98SYjAMQsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaaafa8438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "a = list(itertools.islice(test_loader, index))\n",
    "test_image = a[0][0][0]\n",
    "test_label = a[0][1][0]\n",
    "\n",
    "test_output = model(Variable(test_image.unsqueeze(0)))\n",
    "values, indices = torch.max(test_output, 0)\n",
    "imshow(test_image[0])\n",
    "print(F.softmax(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
